{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cda4e75c-4b40-4836-947a-dae6594855e8",
   "metadata": {},
   "source": [
    "This is a rule-based stemmer. The idea is I get some rules from a grammar book and then add them in here.\n",
    "First, let's get the data from FB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f95cbb69-fc96-42b1-9b34-eacb24a6d8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "corpora_dir = \"../FormosanBank/Corpora\" # this should be the relative path to your FormosanBank download\n",
    "FIND_LANG = \"pyu\" # Puyuma\n",
    "FIND_GLOTTO = \"nanw1237\" # nanwang\n",
    "FIND_DIALECT = \"Nanwang\" # Nanwang\n",
    "consonant_list = ['lr','ng','tr','dr','m','n','p','t','k','b','d','g','s','r','l','y','w'] # used for infixes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebff423-77cc-4242-8ef5-c2895e4a7f6b",
   "metadata": {},
   "source": [
    "If we want to be really sure, there are some other rules we can add in to check the dialect. For instance, in Puyuma if the letter 'b' or the digram 'dr' is present then it's Nanwang (barring loanwords)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd78fbfc-a7da-49cd-90d8-c8d5d183e401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_xmls():\n",
    "# gets all .xml files in corpora_dir.\n",
    "    all_xmls = []\n",
    "    for root, dirname, filenames in os.walk(corpora_dir):\n",
    "        for f in filenames:\n",
    "            if f.endswith(\"xml\"):\n",
    "                all_xmls.append(os.path.join(root,f))\n",
    "    return all_xmls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b31dea0-3dee-4447-b03b-719c9935af0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in a list of xml files and finds which ones match our desired language code(s).\n",
    "# See the formosanbank gitbook for more explanation about the xml format:\n",
    "# https://ai4commsci.gitbook.io/formosanbank/the-bank-architecture/formosanbank-xml-format\n",
    "def get_lang_xmls(file_list, match_lang=FIND_LANG, match_glotto=FIND_GLOTTO, match_dialect=FIND_DIALECT) -> list[str]:\n",
    "    lang_xmls = []\n",
    "    print(f\"Finding xml files with language code {match_lang}, glotto code {match_glotto}, dialect {match_dialect}\")\n",
    "    for filepath in file_list:\n",
    "        tree = ET.parse(filepath)\n",
    "        root = tree.getroot()\n",
    "        if root == None:\n",
    "            print(f\"Unable to parse file: {filepath}\")\n",
    "        # taken from formosanbank validate_xml.py\n",
    "        lang = root.get(\"{http://www.w3.org/XML/1998/namespace}lang\")\n",
    "        if not lang:\n",
    "            # print(f\"{filepath} doesn't appear to have a [lang] attrib: {root.attrib}\")\n",
    "            continue\n",
    "        glottocode = root.get(\"glottocode\")\n",
    "        dialect = root.get(\"dialect\")\n",
    "        if lang.lower() == match_lang.lower():\n",
    "            if not glottocode and not dialect: # If no glotto or dialect, but language matches, add it\n",
    "                # print(f\"glotto: {glottocode} | dialect: {dialect} | file: {' '.join(filepath.split('/')[-5:])}\")\n",
    "                # we assume that just the language code is enough\n",
    "                lang_xmls.append(filepath)\n",
    "            else:\n",
    "                # If glottocode or dialect match, add it\n",
    "                if (glottocode and match_glotto and glottocode.lower() == match_glotto.lower()) or (dialect and match_dialect and dialect.lower() == match_dialect.lower()):\n",
    "                        lang_xmls.append(filepath)\n",
    "    # print(f\"Found language codes: {str(list(set(found_langs)))}\")\n",
    "    # print(f\"Found dialects of {match_lang}: {str(list(set(found_dialects)))}\")\n",
    "    print(f\"Found {len(lang_xmls)} matching xml files\")\n",
    "    if len(lang_xmls) < 6:\n",
    "        for x in lang_xmls:\n",
    "            print('\\t '.join(x.split('/')[3:]))\n",
    "    return lang_xmls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bc8ca7d-a1d3-4156-b42c-a7b94bfb3975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in an xml root, finds all 'sentence' elements, and returns a list of the 'form (standard)' element's text.\n",
    "# See the formosanbank gitbook for more explanation. \n",
    "def get_sent_list(root) -> list[str]:\n",
    "    sents = root.findall(\".//S\")\n",
    "    texts = []\n",
    "    for s in sents:\n",
    "        form_children = []\n",
    "        for child in s:\n",
    "            if child.tag == \"FORM\":\n",
    "                form_children.append(child)\n",
    "            # there is a 'standard' and 'original' form for many sentences. \n",
    "            # If there's only one found, then add the sentence.\n",
    "            # Otherwise, add the 'standard' form.\n",
    "            if len(form_children) == 1:\n",
    "                texts.append(form_children[0].text)\n",
    "            else:\n",
    "                for child in form_children:\n",
    "                    kind = child.get(\"kindOf\")\n",
    "                    if kind == \"standard\":\n",
    "                        texts.append(child.text)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f442471-971a-4976-96bf-fb9e2c3cde3d",
   "metadata": {},
   "source": [
    "Now we have defined our methods, but we haven't used them yet. Let's get all the XML files in the corpora directory, and then filter for our desired language and dialect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29c086be-5e2e-4fc5-a7e6-06cffcd26086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17108\n"
     ]
    }
   ],
   "source": [
    "all_xmls = get_all_xmls()\n",
    "print(len(all_xmls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a93fcb6-a2a3-438c-a1dc-24c7a742faa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding xml files with language code pyu, glotto code nanw1237, dialect Nanwang\n",
      "Found 18 matching xml files\n"
     ]
    }
   ],
   "source": [
    "lang_xmls = get_lang_xmls(all_xmls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad181ab-38e6-4156-aeac-6bd6d7122f6c",
   "metadata": {},
   "source": [
    "We found 18 xml files out of 17,108 in our corpora directory. (Keep in mind that not all of the 17k XML files are language resources.) Now we parse each of these files and get all of the 'sentence' elements in them. We check how many sentences we got, and how many are unique, by printing out the length of the list and set respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a7d4d4d-db7e-4dee-9b45-f3db41029d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84351\n",
      "27078\n"
     ]
    }
   ],
   "source": [
    "sent_list = []\n",
    "for x in lang_xmls:\n",
    "    root = ET.parse(x).getroot()\n",
    "    x_list = get_sent_list(root)\n",
    "    sent_list += x_list\n",
    "\n",
    "print(len(sent_list)) # how many sentences total for our language\n",
    "print(len(set(sent_list))) # how many unique sentences for our language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c99d838-1446-40fb-afe6-87e1efd6f3bb",
   "metadata": {},
   "source": [
    "The difference in the list of all sentences and the 'set' of unique sentences is likely due to many dictionary definitions (i.e., single-word sentences) being included across various dictionaries. Also, it appears some of the ILRDF resources have some sentences repeated across different learning units.\n",
    "\n",
    "Next, we want to collect all the words and sterilize them. We split the sentences into words, and then set the word to lowercase and strip off extra punctuation. The `?` glyph is used as a glottal stop in some languages in FormosanBank (more consistent for displaying instead of the IPA glyph) and the `'` glyph is also used in some languages. For Nanwang, neither are currently used in the orthography, despite glottal stops existing in the language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0d187e7-3e51-4106-aa6a-2dee8f51ef5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "634847\n",
      "Found 48 non-sentences (blank) out of 84351\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "bad_sents = []\n",
    "for s in sent_list:\n",
    "    # If the sentence is empty, then move on\n",
    "    if not s:\n",
    "        bad_sents.append(s)\n",
    "        continue\n",
    "    words = s.split()\n",
    "    for w in words:\n",
    "        w_clean = w.strip(' ,.!\"`~![](){}|/\\\\<>#$@%^&*_-=+?').lower()\n",
    "        if w_clean != '':\n",
    "            # Some words have an apostrophe. Skip them for now.\n",
    "            if w.find(\"'\") != -1:\n",
    "                continue\n",
    "            # It appears that sometimes there's a spacing issue, and two words get glued together by a question mark.\n",
    "            # If so, we split it into two words and then add them both.\n",
    "            if w.find('?') != -1:\n",
    "                fixed_w_list = w.split('?')\n",
    "                for fw in fixed_w_list:\n",
    "                    corpus.append(fw)\n",
    "            else:\n",
    "                corpus.append(w_clean)\n",
    "print(len(corpus))\n",
    "print(f\"Found {str(len(bad_sents))} non-sentences (blank) out of {str(len(sent_list))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "576b8cc4-bc54-4d51-b541-ad2c6458a2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17733\n"
     ]
    }
   ],
   "source": [
    "all_words = set(corpus) # our 'dict' of words we've seen\n",
    "print(len(all_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b397d830-4075-490e-b158-0f9485fc28b6",
   "metadata": {},
   "source": [
    "So far we've read in the corpus for our language, and gotten all the sentences and words. We have a 'dictionary' of valid words (i.e., words that exist in our corpus) that contains 18,622 unique words. If we want to measure word frequency we can use our list of all sentences (or even unique sentences) to calculate.\n",
    "\n",
    "Up until now our methodology has simply been 'reading in the corpus', which isn't very exciting, and is the exact same for any language tool (autocorrect, spell check, stemmer, word prediction, etc). \n",
    "\n",
    "We want to make a word stemmer, so now that we have all our language resources loaded in, we can start by reading the `rules.json` file that defines how words are conjugated in Puyuma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8f04acd-5b66-47ee-a0f9-80efaf1c78da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple affix\n",
      "compound affix\n",
      "reduplication\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "rules_file = \"rules.json\"\n",
    "\n",
    "with open(rules_file, 'r') as f:\n",
    "    rule_data = json.load(f)\n",
    "\n",
    "for top_level in rule_data:\n",
    "    print(top_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a559d5-4c6c-4a23-854e-f04becd8652e",
   "metadata": {},
   "source": [
    "At the highest level, we have simple affixes, compound affixes, and reduplication. Thankfully these are well-organized, so they're not too hard to parse. Let's start with the simple affixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "663f5df4-47ce-4b7b-aabc-b8986bb7c821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple infixes:\n",
      "['in', 'em', 'en', 'um', 'un', 'im']\n",
      "=============\n",
      "Simple prefixes:\n",
      "['be', 'i', 'ku', 'pa', 'ra', 'be', 're', 'ru', 'sa', 'ta', 'ti', 'tu', 'u', 'kan', 'ka', 'ki', 'ma', 'me', 'mi', 'mu', 'ni', 'pa', 'pi', 'pu']\n",
      "=============\n",
      "Simple suffixes:\n",
      "['an', 'aw', 'ay', 'anay', 'u', 'i']\n"
     ]
    }
   ],
   "source": [
    "print('Simple infixes:')\n",
    "infixes = []\n",
    "prefixes = []\n",
    "suffixes = []\n",
    "for rule in rule_data['simple affix']['infix']:\n",
    "    infixes.append(rule['orthography'].strip('-'))\n",
    "print(infixes)\n",
    "\n",
    "print('=============')\n",
    "print('Simple prefixes:')\n",
    "for rule in rule_data['simple affix']['prefix']:\n",
    "    prefixes.append(rule['orthography'].strip('-'))\n",
    "print(prefixes)\n",
    "\n",
    "print('=============')\n",
    "print('Simple suffixes:')\n",
    "for rule in rule_data['simple affix']['suffix']:\n",
    "    suffixes.append(rule['orthography'].strip('-'))\n",
    "print(suffixes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c3e2d4-c625-4a0e-bc46-49b9dd6d67fa",
   "metadata": {},
   "source": [
    "If need be we can print out the 'rule' part of the various affix rule objects, but we already know how simple prefixes and suffixes work, and by reading `rules_json` we also know that an infix only occurs after the first consonant.\n",
    "\n",
    "Below I've written some helper functions to check if a given string matches the given rule, as well as a 'stemming' function if it matches. These are mapped in the `rules_to_methods_dict`, if we wanted to add another rule-based stemming method, all we would need to define is the list of affixes, the way to check if the input string and input affix match, and the way to replace the affix if found.\n",
    "\n",
    "In `get_word_stems` we go through an input list of `candidate_words` and check the appropriate affixes and rules based on the passed in `rule`. If the 'stemmed' word is in our list of `all_words`, then we add it to our output dict as a key, and any words which map to that stem as its values.\n",
    "\n",
    "**Note:** We are assuming here that the stem word exists as its own word. This is not always true across languages, for instance Atayalic languages have a default conjugation that is present if no others are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a9d2975c-ac05-4d41-b0ea-7c02da5f9911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrapper function to have everything play nicely in get_word_stems\n",
    "# All this does is replace the *last* instance of a substring in the input string\n",
    "# Used for stemming a suffix\n",
    "def reverse_replace(input_word, string_to_replace, replace_with, max_replacements):\n",
    "    return replace_with.join(input_word.rsplit(string_to_replace, max_replacements))\n",
    "\n",
    "# Wrapper function to check if the given word has the given infix in the appropriate spot\n",
    "def hasinfix(word_to_check, infix):\n",
    "    for c in consonant_list:\n",
    "        if word_to_check.startswith(c) and word_to_check.replace(c,'',1).startswith(infix):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "rule_to_methods_dict = {\n",
    "    'prefix': {'affix_list': prefixes, 'check_function' : str.startswith},\n",
    "    'suffix': {'affix_list': suffixes, 'check_function' : str.endswith, 'replace_function': reverse_replace},\n",
    "    'infix':  {'affix_list': infixes,  'check_function': str.startswith},\n",
    "}\n",
    "\n",
    "# Takes in a list of candidate words and the corresponding rule\n",
    "# Sets the 'stemming function' (replace_func) and affix_list based on which rule we are looking at\n",
    "# Returns a dictionary of {stem: [words, which, map, to, stem] ...}\n",
    "def get_word_stems(candidate_words, rule):\n",
    "    stem_affix_dict = {}\n",
    "    affix_list = rule_to_methods_dict[rule]['affix_list']\n",
    "    check_function = rule_to_methods_dict[rule]['check_function']\n",
    "    replace_function = rule_to_methods_dict[rule].get('replace_function', str.replace)\n",
    "        \n",
    "    for cand in candidate_words:\n",
    "        for affix in affix_list:\n",
    "            if check_function(cand, affix):\n",
    "                stem = replace_function(cand, affix, '', 1)\n",
    "                if stem in all_words:\n",
    "                    if stem not in stem_affix_dict:\n",
    "                        stem_affix_dict[stem] =[]\n",
    "                    if cand not in stem_affix_dict[stem]:\n",
    "                        stem_affix_dict[stem].append(cand)\n",
    "\n",
    "    return stem_affix_dict\n",
    "\n",
    "# Finally, we need a helper function that will take two dictionaries, and return a dictionary that is a combination of the two\n",
    "# We write our own function because we need the values which are lists to be joined, not overwritten\n",
    "def merge_two_dicts(base_dict, update_dict):\n",
    "    ret_dict = base_dict\n",
    "    for k in update_dict.keys():\n",
    "        if k in base_dict.keys():\n",
    "            vals = set(base_dict[k])\n",
    "            vals.update(update_dict[k])\n",
    "            ret_dict[k] = list(vals)\n",
    "        else:\n",
    "            ret_dict[k] = update_dict[k]\n",
    "    return ret_dict\n",
    "\n",
    "# returns the first x keys and values of dict as a string\n",
    "# if x is not given, returns the entire dict\n",
    "def get_x_dict_entries(input_dict, x=-1):\n",
    "    i = 0\n",
    "    ret_str = \"\"\n",
    "    for k in input_dict.keys():\n",
    "        ret_str += f\"Stem: {k}\"\n",
    "        ret_str += \"\\n\"\n",
    "        ret_str += f\"Base words: {input_dict[k]}\"\n",
    "        ret_str += \"\\n\\n\"\n",
    "        i += 1\n",
    "        if i == x:\n",
    "            break\n",
    "    print(f\"Got {i} entries from input dict\")\n",
    "    return ret_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51ed6e1-2218-4345-be14-eb9a28562ca6",
   "metadata": {},
   "source": [
    "With these functions defined, we can now do multiple passes. This is because some words might have a prefix, suffix, and infix, or it could be even more complex with an infix in the middle of a prefix. The methodology is essentially:\n",
    "1. Get 1-step stems from all_words\n",
    "2. For all the 1-step stems, check if they can be 'stemmed' again based on the above rules\n",
    "3. If so, then create a dict of 2-step stems using the 1-step stems as input\n",
    "4. Repeat until we no longer find any new 'stems'\n",
    "5. Once we stop finding new 'stems', we remove all the keys (stems) which occur as values (base words)\n",
    "\n",
    "With all our helper functions defined, we will execute this process in code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fb7f609f-dfa5-4e50-aee9-d7cd5f9bef00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the iterative loop with 17733 words to check\n",
      "Found 1877 candidate words for rule 'prefix'\n",
      "Found 1865 candidate words for rule 'suffix'\n",
      "Found 134 candidate words for rule 'infix'\n",
      "Found a total of 3194 1-step stems\n",
      "Now there are 3194 words to check after iteration: 1\n",
      "\n",
      "Found 332 candidate words for rule 'prefix'\n",
      "Found 396 candidate words for rule 'suffix'\n",
      "Found 30 candidate words for rule 'infix'\n",
      "Found a total of 631 2-step stems\n",
      "Now there are 631 words to check after iteration: 2\n",
      "\n",
      "Found 59 candidate words for rule 'prefix'\n",
      "Found 35 candidate words for rule 'suffix'\n",
      "Found 7 candidate words for rule 'infix'\n",
      "Found a total of 86 3-step stems\n",
      "Now there are 86 words to check after iteration: 3\n",
      "\n",
      "Found 4 candidate words for rule 'prefix'\n",
      "Found 3 candidate words for rule 'suffix'\n",
      "Found 1 candidate words for rule 'infix'\n",
      "Found a total of 6 4-step stems\n",
      "Now there are 6 words to check after iteration: 4\n",
      "\n",
      "Found 0 candidate words for rule 'prefix'\n",
      "Found 0 candidate words for rule 'suffix'\n",
      "Found 0 candidate words for rule 'infix'\n",
      "Found a total of 0 5-step stems\n",
      "Now there are 0 words to check after iteration: 5\n",
      "\n",
      "There is a total of 3194 potential stems\n",
      "There is a total of 5214 base words to make those stems\n",
      "\n",
      "Removing all base words from the stem dict\n",
      "Final stem dict has a length of 2288\n",
      "First 10 entries:\n",
      "Got 10 entries from input dict\n",
      "Stem: bowaro\n",
      "Base words: ['pabowaro']\n",
      "\n",
      "Stem: beliyas\n",
      "Base words: ['beliyasu', 'mubeliyas', 'beliyasaw']\n",
      "\n",
      "Stem: talram\n",
      "Base words: ['talrami', 'talramay', 'tatalram', 'talraman']\n",
      "\n",
      "Stem: yaulas\n",
      "Base words: ['piyaulas', 'miyaulas']\n",
      "\n",
      "Stem: isatr\n",
      "Base words: ['uisatr', 'puisatr', 'muisatr']\n",
      "\n",
      "Stem: laus\n",
      "Base words: ['lausu', 'lausaw', 'melaus', 'mulaus', 'nilaus', 'lausi']\n",
      "\n",
      "Stem: butru\n",
      "Base words: ['mibutru', 'ubutru']\n",
      "\n",
      "Stem: rebu\n",
      "Base words: ['tarebu', 'purebu']\n",
      "\n",
      "Stem: nao\n",
      "Base words: ['panao', 'menao']\n",
      "\n",
      "Stem: senan\n",
      "Base words: ['pasenan', 'senanaw']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This will be the dictionary that we continually update with new stems and their base words\n",
    "all_stems_to_bases_dict = {}\n",
    "\n",
    "# This will be the list of all base words. It is useful to have as its own continuous list so that we can iterate through it later\n",
    "all_base_words = set()\n",
    "\n",
    "# To begin, we need to check the '0-step stems', i.e. all the words in our corpus\n",
    "words_to_check = all_words\n",
    "\n",
    "print(f\"Starting the iterative loop with {len(all_words)} words to check\")\n",
    "i = 0\n",
    "while len(words_to_check) != 0:\n",
    "    i += 1\n",
    "    if i >= 100:\n",
    "        print(\"Uh oh, did you make an infinite loop?\")\n",
    "        break\n",
    "    # This stores the k-step stems and their base words for each step\n",
    "    stem_base_update_dict = {}\n",
    "    for r in rule_to_methods_dict.keys():\n",
    "        rule_candidate_dict = get_word_stems(words_to_check, r)\n",
    "        print(f\"Found {len(rule_candidate_dict)} candidate words for rule '{r}'\")\n",
    "        stem_base_update_dict = merge_two_dicts(stem_base_update_dict, rule_candidate_dict)\n",
    "    print(f\"Found a total of {len(stem_base_update_dict)} {i}-step stems\")\n",
    "    \n",
    "    all_stems_to_bases_dict = merge_two_dicts(all_stems_to_bases_dict, stem_base_update_dict)\n",
    "    for k in stem_base_update_dict.keys():\n",
    "        all_base_words.update(stem_base_update_dict[k])\n",
    "    words_to_check = stem_base_update_dict.keys()\n",
    "    print(f\"Now there are {len(words_to_check)} words to check after iteration: {i}\")\n",
    "    print(\"\") # newline\n",
    "\n",
    "print(f\"There is a total of {len(all_stems_to_bases_dict)} potential stems\")\n",
    "print(f\"There is a total of {len(all_base_words)} base words to make those stems\")\n",
    "print(\"\")\n",
    "print(\"Removing all base words from the stem dict\")\n",
    "final_stem_dict = {}\n",
    "for k in all_stems_to_bases_dict:\n",
    "    if k not in all_base_words:\n",
    "        final_stem_dict[k] = all_stems_to_bases_dict[k]\n",
    "\n",
    "print(f\"Final stem dict has a length of {len(final_stem_dict)}\")\n",
    "print(f\"First {amount_to_print} entries:\")\n",
    "print_str = get_x_dict_entries(final_stem_dict, amount_to_print)\n",
    "print(print_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e261f571-86dc-4952-a72c-22ca42fe7a21",
   "metadata": {},
   "source": [
    "Finally, let's save our dict to a .txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f2845950-182b-41c7-ae1d-9fa26d3a2045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 2288 entries from input dict\n",
      "Saved as stem_base_words.txt\n"
     ]
    }
   ],
   "source": [
    "save_name = \"stem_base_words.txt\"\n",
    "save_str = get_x_dict_entries(final_stem_dict)\n",
    "with open(save_name, 'w') as f:\n",
    "    f.write(save_str)\n",
    "print(f\"Saved as {save_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e7e78f-093c-4518-ab7e-d8a7a026e62a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
