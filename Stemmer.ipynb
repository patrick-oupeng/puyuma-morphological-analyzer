{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cda4e75c-4b40-4836-947a-dae6594855e8",
   "metadata": {},
   "source": [
    "This is a rule-based stemmer. The idea is I get some rules from a grammar book and then add them in here.\n",
    "First, let's get the data from FB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f95cbb69-fc96-42b1-9b34-eacb24a6d8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "corpora_dir = \"../FormosanBank/Corpora\" # this should be the relative path to your FormosanBank download\n",
    "FIND_LANG = \"pyu\" # Puyuma\n",
    "FIND_GLOTTO = \"nanw1237\" # nanwang\n",
    "FIND_DIALECT = \"Nanwang\" # Nanwang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd78fbfc-a7da-49cd-90d8-c8d5d183e401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_xmls():\n",
    "# gets all .xml files in corpora_dir.\n",
    "    all_xmls = []\n",
    "    for root, dirname, filenames in os.walk(corpora_dir):\n",
    "        for f in filenames:\n",
    "            if f.endswith(\"xml\"):\n",
    "                all_xmls.append(os.path.join(root,f))\n",
    "    return all_xmls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b31dea0-3dee-4447-b03b-719c9935af0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lang_xmls(file_list, match_lang=FIND_LANG, match_glotto=FIND_GLOTTO, match_dialect=FIND_DIALECT) -> list[str]:\n",
    "# takes in a list of xml files and finds which ones match our desired language code(s).\n",
    "    lang_xmls = []\n",
    "    print(f\"Finding xml files with language code {match_lang}, glotto code {match_glotto}, dialect {match_dialect}\")\n",
    "    for filepath in file_list:\n",
    "        tree = ET.parse(filepath)\n",
    "        root = tree.getroot()\n",
    "        if root == None:\n",
    "            print(f\"Unable to parse file: {filepath}\")\n",
    "        # taken from formosanbank validate_xml.py\n",
    "        lang = root.get(\"{http://www.w3.org/XML/1998/namespace}lang\")\n",
    "        if not lang:\n",
    "            # print(f\"{filepath} doesn't appear to have a [lang] attrib: {root.attrib}\")\n",
    "            continue\n",
    "        glottocode = root.get(\"glottocode\")\n",
    "        dialect = root.get(\"dialect\")\n",
    "        if lang.lower() == match_lang.lower():\n",
    "            if not glottocode and not dialect: # If no glotto or dialect, but language matches, add it\n",
    "                # print(f\"glotto: {glottocode} | dialect: {dialect} | file: {' '.join(filepath.split('/')[-5:])}\")\n",
    "                # we assume that just the language code is enough\n",
    "                lang_xmls.append(filepath)\n",
    "            else:\n",
    "                # If glottocode or dialect match, add it\n",
    "                if (glottocode and match_glotto and glottocode.lower() == match_glotto.lower()) or (dialect and match_dialect and dialect.lower() == match_dialect.lower()):\n",
    "                        lang_xmls.append(filepath)\n",
    "    # print(f\"Found language codes: {str(list(set(found_langs)))}\")\n",
    "    # print(f\"Found dialects of {match_lang}: {str(list(set(found_dialects)))}\")\n",
    "    print(f\"Found {len(lang_xmls)} matching xml files\")\n",
    "    if len(lang_xmls) < 6:\n",
    "        for x in lang_xmls:\n",
    "            print('\\t '.join(x.split('/')[3:]))\n",
    "    return lang_xmls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bc8ca7d-a1d3-4156-b42c-a7b94bfb3975",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sent_list(root) -> list[str]:\n",
    "# takes in an xml root, finds all 'sentence' elements, and returns a list of the 'form (standard)' element's text\n",
    "    sents = root.findall(\".//S\")\n",
    "    texts = []\n",
    "    for s in sents:\n",
    "        form_children = []\n",
    "        for child in s:\n",
    "            if child.tag == \"FORM\":\n",
    "                form_children.append(child)\n",
    "            # there is 'standard' and 'original' forms\n",
    "            if len(form_children) == 1:\n",
    "                texts.append(form_children[0].text)\n",
    "            else:\n",
    "                for child in form_children:\n",
    "                    kind = child.get(\"kindOf\")\n",
    "                    if kind == \"standard\":\n",
    "                        texts.append(child.text)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29c086be-5e2e-4fc5-a7e6-06cffcd26086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17108\n"
     ]
    }
   ],
   "source": [
    "all_xmls = get_all_xmls()\n",
    "print(len(all_xmls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a93fcb6-a2a3-438c-a1dc-24c7a742faa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding xml files with language code pyu, glotto code nanw1237, dialect Nanwang\n",
      "Found 18 matching xml files\n"
     ]
    }
   ],
   "source": [
    "lang_xmls = get_lang_xmls(all_xmls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a7d4d4d-db7e-4dee-9b45-f3db41029d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84351\n",
      "27078\n"
     ]
    }
   ],
   "source": [
    "sent_list = []\n",
    "for x in lang_xmls:\n",
    "    root = ET.parse(x).getroot()\n",
    "    x_list = get_sent_list(root)\n",
    "    sent_list += x_list\n",
    "print(len(sent_list))\n",
    "# print(len(list(Set(sent_list))))\n",
    "# print(len(sent_list.Set()))\n",
    "print(len(set(sent_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c99d838-1446-40fb-afe6-87e1efd6f3bb",
   "metadata": {},
   "source": [
    "The difference in the pure list of sentences and the 'set' of sentences is likely due to many dictionary definitions (i.e., single-word sentences) being included across various dictionaries. Also, as mentioned in another notebook, ILRDF has some sentences repeated across different learning units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e0d187e7-3e51-4106-aa6a-2dee8f51ef5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "623347\n",
      "Found 48 non-sentences (blank) out of 84351\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "bad_sents = []\n",
    "for s in sent_list:\n",
    "    if not s:\n",
    "        bad_sents.append(s)\n",
    "        continue\n",
    "    words = s.split()\n",
    "    for w in words:\n",
    "        w_clean = w.strip(' ,.!\"`~![](){}|/\\\\<>#$@%^&*_-=+').lower()\n",
    "        if w_clean != '':\n",
    "            corpus.append(w_clean)\n",
    "print(len(corpus))\n",
    "print(f\"Found {str(len(bad_sents))} non-sentences (blank) out of {str(len(sent_list))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "576b8cc4-bc54-4d51-b541-ad2c6458a2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18622\n"
     ]
    }
   ],
   "source": [
    "all_words = set(corpus) # our 'dict' of words we've seen\n",
    "print(len(all_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b397d830-4075-490e-b158-0f9485fc28b6",
   "metadata": {},
   "source": [
    "Up until now our methodology has simply been 'reading in the corpus', which isn't very exciting, and is the exact same for any of the language tools we want to make. Next, we will load in the `rules.json` file for stemming rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a8f04acd-5b66-47ee-a0f9-80efaf1c78da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple affix\n",
      "compound affix\n",
      "reduplication\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "rules_file = \"rules.json\"\n",
    "\n",
    "with open(rules_file, 'r') as f:\n",
    "    rule_data = json.load(f)\n",
    "\n",
    "for top_level in rule_data:\n",
    "    print(top_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a559d5-4c6c-4a23-854e-f04becd8652e",
   "metadata": {},
   "source": [
    "At the highest level, we have simple affixes, compound affixes, and reduplication. Thankfully these are well-organized, so they're not too hard to parse. Let's start with the simple affixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "663f5df4-47ce-4b7b-aabc-b8986bb7c821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infix\n",
      "prefix\n",
      "suffix\n"
     ]
    }
   ],
   "source": [
    "for simp_rule in rule_data['simple affix']:\n",
    "    print(simp_rule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f644c4e9-3d43-48aa-8b85-a93612ebcbb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['in', 'em', 'en', 'um', 'un', 'im']\n"
     ]
    }
   ],
   "source": [
    "infix_orths = []\n",
    "for infix_rule in rule_data['simple affix']['infix']:\n",
    "    infix_orths.append(infix_rule['orthography'].strip('-'))\n",
    "print(infix_orths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c3e2d4-c625-4a0e-bc46-49b9dd6d67fa",
   "metadata": {},
   "source": [
    "If need be we can print out the 'rule' part of the infix_rule object, but it looks like we have a series of infixes, and they all occur after the first consonant. Let's write a simple program to check through our 'dictionary' `all_words` and find any words that fit this pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7ac2987c-f400-41dd-84dc-40641bd3e0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2242\n"
     ]
    }
   ],
   "source": [
    "consonant_list = 'bcdfghjklmnpqrstvwxz?'\n",
    "candidate_words = []\n",
    "for w in all_words:\n",
    "    if (w[0] in consonant_list and w[1:3] in infix_orths):\n",
    "        # print(w)\n",
    "        candidate_words.append(w)\n",
    "print(len(candidate_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a9d2975c-ac05-4d41-b0ea-7c02da5f9911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "674\n"
     ]
    }
   ],
   "source": [
    "candidate_stem_dict = {}\n",
    "for cand in candidate_words:\n",
    "    stem = cand[0] + cand[3:]\n",
    "    if stem in all_words:\n",
    "        if stem not in candidate_stem_dict:\n",
    "            candidate_stem_dict[stem] = []\n",
    "        candidate_stem_dict[stem].append(cand)\n",
    "print(len(candidate_stem_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "854ed39f-4214-46b5-a320-bad4378eca92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate words for stem: \n",
      "\tkakeris\n",
      "\t['kemakeris']\n",
      "\n",
      "Candidate words for stem: \n",
      "\tketket\n",
      "\t['kemetket', 'kinetket']\n",
      "\n",
      "Candidate words for stem: \n",
      "\tmao\n",
      "\t['menao']\n",
      "\n",
      "Candidate words for stem: \n",
      "\tpatedrelr\n",
      "\t['pinatedrelr', 'penatedrelr']\n",
      "\n",
      "Candidate words for stem: \n",
      "\tbalruk\n",
      "\t['benalruk']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for stem in candidate_stem_dict:\n",
    "    print(f\"Candidate words for stem: \\n\\t{stem}\")\n",
    "    print(f\"\\t{candidate_stem_dict[stem]}\")\n",
    "    print(\"\")\n",
    "    i += 1\n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e026c3f-962a-447e-8b86-08b809767b18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
