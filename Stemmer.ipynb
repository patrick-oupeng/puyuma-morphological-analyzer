{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cda4e75c-4b40-4836-947a-dae6594855e8",
   "metadata": {},
   "source": [
    "This is a rule-based stemmer. The idea is I get some rules from a grammar book and then add them in here.\n",
    "First, let's get the data from FB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f95cbb69-fc96-42b1-9b34-eacb24a6d8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "corpora_dir = \"../FormosanBank/Corpora\" # this should be the relative path to your FormosanBank download\n",
    "FIND_LANG = \"pyu\" # Puyuma\n",
    "FIND_GLOTTO = \"nanw1237\" # nanwang\n",
    "FIND_DIALECT = \"Nanwang\" # Nanwang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd78fbfc-a7da-49cd-90d8-c8d5d183e401",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_xmls():\n",
    "# gets all .xml files in corpora_dir.\n",
    "    all_xmls = []\n",
    "    for root, dirname, filenames in os.walk(corpora_dir):\n",
    "        for f in filenames:\n",
    "            if f.endswith(\"xml\"):\n",
    "                all_xmls.append(os.path.join(root,f))\n",
    "    return all_xmls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b31dea0-3dee-4447-b03b-719c9935af0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in a list of xml files and finds which ones match our desired language code(s).\n",
    "# See the formosanbank gitbook for more explanation about the xml format:\n",
    "# https://ai4commsci.gitbook.io/formosanbank/the-bank-architecture/formosanbank-xml-format\n",
    "def get_lang_xmls(file_list, match_lang=FIND_LANG, match_glotto=FIND_GLOTTO, match_dialect=FIND_DIALECT) -> list[str]:\n",
    "    lang_xmls = []\n",
    "    print(f\"Finding xml files with language code {match_lang}, glotto code {match_glotto}, dialect {match_dialect}\")\n",
    "    for filepath in file_list:\n",
    "        tree = ET.parse(filepath)\n",
    "        root = tree.getroot()\n",
    "        if root == None:\n",
    "            print(f\"Unable to parse file: {filepath}\")\n",
    "        # taken from formosanbank validate_xml.py\n",
    "        lang = root.get(\"{http://www.w3.org/XML/1998/namespace}lang\")\n",
    "        if not lang:\n",
    "            # print(f\"{filepath} doesn't appear to have a [lang] attrib: {root.attrib}\")\n",
    "            continue\n",
    "        glottocode = root.get(\"glottocode\")\n",
    "        dialect = root.get(\"dialect\")\n",
    "        if lang.lower() == match_lang.lower():\n",
    "            if not glottocode and not dialect: # If no glotto or dialect, but language matches, add it\n",
    "                # print(f\"glotto: {glottocode} | dialect: {dialect} | file: {' '.join(filepath.split('/')[-5:])}\")\n",
    "                # we assume that just the language code is enough\n",
    "                lang_xmls.append(filepath)\n",
    "            else:\n",
    "                # If glottocode or dialect match, add it\n",
    "                if (glottocode and match_glotto and glottocode.lower() == match_glotto.lower()) or (dialect and match_dialect and dialect.lower() == match_dialect.lower()):\n",
    "                        lang_xmls.append(filepath)\n",
    "    # print(f\"Found language codes: {str(list(set(found_langs)))}\")\n",
    "    # print(f\"Found dialects of {match_lang}: {str(list(set(found_dialects)))}\")\n",
    "    print(f\"Found {len(lang_xmls)} matching xml files\")\n",
    "    if len(lang_xmls) < 6:\n",
    "        for x in lang_xmls:\n",
    "            print('\\t '.join(x.split('/')[3:]))\n",
    "    return lang_xmls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bc8ca7d-a1d3-4156-b42c-a7b94bfb3975",
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes in an xml root, finds all 'sentence' elements, and returns a list of the 'form (standard)' element's text.\n",
    "# See the formosanbank gitbook for more explanation. \n",
    "def get_sent_list(root) -> list[str]:\n",
    "    sents = root.findall(\".//S\")\n",
    "    texts = []\n",
    "    for s in sents:\n",
    "        form_children = []\n",
    "        for child in s:\n",
    "            if child.tag == \"FORM\":\n",
    "                form_children.append(child)\n",
    "            # there is a 'standard' and 'original' form for many sentences. \n",
    "            # If there's only one found, then add the sentence.\n",
    "            # Otherwise, add the 'standard' form.\n",
    "            if len(form_children) == 1:\n",
    "                texts.append(form_children[0].text)\n",
    "            else:\n",
    "                for child in form_children:\n",
    "                    kind = child.get(\"kindOf\")\n",
    "                    if kind == \"standard\":\n",
    "                        texts.append(child.text)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f442471-971a-4976-96bf-fb9e2c3cde3d",
   "metadata": {},
   "source": [
    "Now we have defined our methods, but we haven't used them yet. Let's get all the XML files in the corpora directory, and then filter for our desired language and dialect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "29c086be-5e2e-4fc5-a7e6-06cffcd26086",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17108\n"
     ]
    }
   ],
   "source": [
    "all_xmls = get_all_xmls()\n",
    "print(len(all_xmls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a93fcb6-a2a3-438c-a1dc-24c7a742faa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding xml files with language code pyu, glotto code nanw1237, dialect Nanwang\n",
      "Found 18 matching xml files\n"
     ]
    }
   ],
   "source": [
    "lang_xmls = get_lang_xmls(all_xmls)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ad181ab-38e6-4156-aeac-6bd6d7122f6c",
   "metadata": {},
   "source": [
    "We found 18 xml files out of 17,108 in our corpora directory. (Keep in mind that not all of the 17k XML files are language resources.) Now we parse each of these files and get all of the 'sentence' elements in them. We check how many sentences we got, and how many are unique, by printing out the length of the list and set respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a7d4d4d-db7e-4dee-9b45-f3db41029d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84351\n",
      "27078\n"
     ]
    }
   ],
   "source": [
    "sent_list = []\n",
    "for x in lang_xmls:\n",
    "    root = ET.parse(x).getroot()\n",
    "    x_list = get_sent_list(root)\n",
    "    sent_list += x_list\n",
    "\n",
    "print(len(sent_list)) # how many sentences total for our language\n",
    "print(len(set(sent_list))) # how many unique sentences for our language"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c99d838-1446-40fb-afe6-87e1efd6f3bb",
   "metadata": {},
   "source": [
    "The difference in the list of all sentences and the 'set' of unique sentences is likely due to many dictionary definitions (i.e., single-word sentences) being included across various dictionaries. Also, it appears some of the ILRDF resources have some sentences repeated across different learning units.\n",
    "\n",
    "Next, we want to collect all the words and sterilize them. We split the sentences into words, and then set the word to lowercase and strip off extra punctuation. (Note that the `?` glyph is used as the glottal stop in some languages.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e0d187e7-3e51-4106-aa6a-2dee8f51ef5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "623347\n",
      "Found 48 non-sentences (blank) out of 84351\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "bad_sents = []\n",
    "for s in sent_list:\n",
    "    # If the sentence is empty, then move on\n",
    "    if not s:\n",
    "        bad_sents.append(s)\n",
    "        continue\n",
    "    words = s.split()\n",
    "    for w in words:\n",
    "        w_clean = w.strip(' ,.!\"`~![](){}|/\\\\<>#$@%^&*_-=+').lower()\n",
    "        if w_clean != '':\n",
    "            corpus.append(w_clean)\n",
    "print(len(corpus))\n",
    "print(f\"Found {str(len(bad_sents))} non-sentences (blank) out of {str(len(sent_list))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "576b8cc4-bc54-4d51-b541-ad2c6458a2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18622\n"
     ]
    }
   ],
   "source": [
    "all_words = set(corpus) # our 'dict' of words we've seen\n",
    "print(len(all_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b397d830-4075-490e-b158-0f9485fc28b6",
   "metadata": {},
   "source": [
    "So far we've read in the corpus for our language, and gotten all the sentences and words. We have a 'dictionary' of valid words (i.e., words that exist in our corpus) that contains 18,622 unique words. If we want to measure word frequency we can use our list of all sentences (or even unique sentences) to calculate.\n",
    "\n",
    "Up until now our methodology has simply been 'reading in the corpus', which isn't very exciting, and is the exact same for any language tool (autocorrect, spell check, stemmer, word prediction, etc). \n",
    "\n",
    "We want to make a word stemmer, so now that we have all our language resources loaded in, we can start by reading the `rules.json` file that defines how words are conjugated in Puyuma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8f04acd-5b66-47ee-a0f9-80efaf1c78da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simple affix\n",
      "compound affix\n",
      "reduplication\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "rules_file = \"rules.json\"\n",
    "\n",
    "with open(rules_file, 'r') as f:\n",
    "    rule_data = json.load(f)\n",
    "\n",
    "for top_level in rule_data:\n",
    "    print(top_level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a559d5-4c6c-4a23-854e-f04becd8652e",
   "metadata": {},
   "source": [
    "At the highest level, we have simple affixes, compound affixes, and reduplication. Thankfully these are well-organized, so they're not too hard to parse. Let's start with the simple affixes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "663f5df4-47ce-4b7b-aabc-b8986bb7c821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infix\n",
      "prefix\n",
      "suffix\n",
      "=============\n",
      "Simple infixes:\n",
      "['in', 'em', 'en', 'um', 'un', 'im']\n"
     ]
    }
   ],
   "source": [
    "for simp_rule in rule_data['simple affix']:\n",
    "    print(simp_rule)\n",
    "print('=============')\n",
    "print('Simple infixes:')\n",
    "infix_orths = []\n",
    "for infix_rule in rule_data['simple affix']['infix']:\n",
    "    infix_orths.append(infix_rule['orthography'].strip('-'))\n",
    "print(infix_orths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c3e2d4-c625-4a0e-bc46-49b9dd6d67fa",
   "metadata": {},
   "source": [
    "If need be we can print out the 'rule' part of the infix_rule object, but it looks like we have a series of infixes, and they all occur after the first consonant. Let's write a simple program to check through our 'dictionary' `all_words` and find any words that fit this pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ac2987c-f400-41dd-84dc-40641bd3e0a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2242\n"
     ]
    }
   ],
   "source": [
    "consonant_list = 'bcdfghjklmnpqrstvwxz?'\n",
    "candidate_words = []\n",
    "for w in all_words:\n",
    "    if (w[0] in consonant_list and w[1:3] in infix_orths):\n",
    "        # print(w)\n",
    "        candidate_words.append(w)\n",
    "print(len(candidate_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7da458c-ed61-48bb-8a2c-b71e67bb138e",
   "metadata": {},
   "source": [
    "In our entire list of words, there are 2,242 that match the pattern of `C-IF-` where `C` is an initial consonant and `IF` is any of our simple infixes.\n",
    "\n",
    "Now let's go through all of those 2,242 candidates, and try to remove the infix, and see if the resulting 'stem word' exists in our dictionary of `all_words`. \n",
    "\n",
    "**Note:** we are assuming here that the stem word exists as its own word. This is not always true across languages, for instance Atayalic languages have a default conjugation that is present if no others are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a9d2975c-ac05-4d41-b0ea-7c02da5f9911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "676\n"
     ]
    }
   ],
   "source": [
    "candidate_stem_dict = {}\n",
    "for cand in candidate_words:\n",
    "    stem = cand[0] + cand[3:]\n",
    "    if stem in all_words:\n",
    "        if stem not in candidate_stem_dict:\n",
    "            candidate_stem_dict[stem] = []\n",
    "        candidate_stem_dict[stem].append(cand)\n",
    "print(len(candidate_stem_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a464e3a4-79a1-48a8-8136-08af96dd88fb",
   "metadata": {},
   "source": [
    "Great, out of our 2,242 words, we found 676 stems after removing the affixes. Let's print out some of them and check our work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "854ed39f-4214-46b5-a320-bad4378eca92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate words for stem: \n",
      "\ttodro\n",
      "\t['temodro']\n",
      "\n",
      "Candidate words for stem: \n",
      "\tbetreel\n",
      "\t['binetreel', 'benetreel']\n",
      "\n",
      "Candidate words for stem: \n",
      "\tturu\n",
      "\t['temuru']\n",
      "\n",
      "Candidate words for stem: \n",
      "\tkakawang\n",
      "\t['kemakawang']\n",
      "\n",
      "Candidate words for stem: \n",
      "\tpaallupan\n",
      "\t['pinaallupan']\n",
      "\n",
      "Candidate words for stem: \n",
      "\tsuru\n",
      "\t['semuru']\n",
      "\n",
      "Candidate words for stem: \n",
      "\tkurkur\n",
      "\t['kemurkur']\n",
      "\n",
      "Candidate words for stem: \n",
      "\ttililr\n",
      "\t['tinililr', 'temililr']\n",
      "\n",
      "Candidate words for stem: \n",
      "\tsoang\n",
      "\t['semoang']\n",
      "\n",
      "Candidate words for stem: \n",
      "\tkirepauwayan\n",
      "\t['kinirepauwayan']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for stem in candidate_stem_dict:\n",
    "    print(f\"Candidate words for stem: \\n\\t{stem}\")\n",
    "    print(f\"\\t{candidate_stem_dict[stem]}\")\n",
    "    print(\"\")\n",
    "    i += 1\n",
    "    if i == 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6ca3b7-7263-4594-8081-973221fad4a1",
   "metadata": {},
   "source": [
    "So far this was a simple tutorial showing how we might use a rules file to parse a corpus into basic stems. There are many improvements that could be made. Ultimately we would probably want to build a separate list of 'stems' that we check against, and slowly bootstrap the functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ee5646-0b72-4ecf-a768-37b978b978d1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
